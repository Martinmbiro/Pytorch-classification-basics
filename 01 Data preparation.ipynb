{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+D1Z6Be8mVkHKy6gDAZyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martinmbiro/Pytorch-classification-basics/blob/main/01%20Data%20preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preparation**\n",
        "> For this exercise, I'll be using the [`sklearn's`](https://scikit-learn.org/stable/index.html) famous [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) to train a Neural Network for multi-class classification"
      ],
      "metadata": {
        "id": "587Sa90XksQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data\n",
        "> ✋ **Info**\n",
        "+ The dataset contains a set of 150 records with four features: `sepal length`, `sepal width`, `petal length`, `petal width` that attribute to three species of Iris, `setosa`, `versicolor`, & `virginica`\n",
        "+ Calling [`load_iris(return_X_y=False, as_frame=False)`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) on [`sklearn.dataset`](https://scikit-learn.org/stable/api/sklearn.datasets.html) returns a [`sklearn.utils.Bunch`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch) object that extends the normal python [`dictionary`](https://www.w3schools.com/python/python_dictionaries.asp) but with additional capabilities\n",
        "+ If `as_frame=True`, `data` will be a `pandas` [`DataFrame`](https://pandas.pydata.org/docs/reference/frame.html#dataframe), otherwise, a `NumPy` [`ndarray`](https://numpy.org/doc/stable/reference/arrays.ndarray.html#the-n-dimensional-array-ndarray). For the sake of practice, I'll be working with an `ndarray`"
      ],
      "metadata": {
        "id": "ZsX1KqGXp07V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDWLAC20xX-T"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "# load iris dataset\n",
        "iris = datasets.load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# return bunch keys\n",
        "print(f'keys:\\n{iris.keys()}\\n')\n",
        "# print column names\n",
        "print(f'feature_names:\\n{iris.feature_names}\\n')\n",
        "# target names\n",
        "print(f'target_names:\\n{iris.target_names}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsy9OncYj8l5",
        "outputId": "94f727fb-6a98-4bb7-87a1-aee43e78745d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys:\n",
            "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
            "\n",
            "feature_names:\n",
            "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "\n",
            "target_names:\n",
            "['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the shape\n",
        "iris.data.shape, iris.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFc1py5WxqBZ",
        "outputId": "45885a74-4228-48a3-8fb9-c6fc69d20a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 4), (150,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process the data\n",
        "> Here, I'll build a scikit learn [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#pipeline) for preprocessing the data. Part of this will entail scaling, since this is a standard practice in Machine Learning in general  \n",
        "\n",
        "> 🔔 **Info**\n",
        "+ [`SimplImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#simpleimputer) - Replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column, or using a constant value\n",
        "+ [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#minmaxscaler) - Transform features by scaling each feature to a given range, by default `(0, 1)`\n",
        "+ [`make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#make-pipeline) - Construct a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#pipeline) for preprocessing the data from given _estimators_"
      ],
      "metadata": {
        "id": "EdYaj0CistqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "pzPm36kZuYp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make preprocessing pileline with inputer & minmaxscaler\n",
        "preprocessor = make_pipeline(\n",
        "  SimpleImputer(strategy='median'), # handle missing values, if any\n",
        "  MinMaxScaler() # scale data on a scale of (0,1)\n",
        ")"
      ],
      "metadata": {
        "id": "PN8qDi8kucpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the pipeline on data and view 1st 10 rows:\n",
        "preprocessor.fit_transform(iris.data)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCc9QacmuTNf",
        "outputId": "5a149b83-276d-4c6e-8521-915745f7a15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
              "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
              "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
              "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
              "       [0.19444444, 0.66666667, 0.06779661, 0.04166667]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ▶️ **Up Next**\n",
        "\n",
        "> Loading data and model building"
      ],
      "metadata": {
        "id": "uB9QsIZF7gjy"
      }
    }
  ]
}